{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence  # padding our sequences\n",
    "\n",
    "\n",
    "from DatasetClasses import TweetEmbeddingDataset\n",
    "from LSTM_Models import LSTM_EMB_T    # LSTM (input embeddings, time scalars) -> list[0,1,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819afedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_embedding_path = \"/home/public/tweetdatanlp/sent-trans-dbs/tweet_embeddings.npy\"\n",
    "meta_path = \"/home/public/tweetdatanlp/sent-trans-dbs/tweet_metadata.csv\"\n",
    "\n",
    "# init dataset\n",
    "data=TweetEmbeddingDataset(sen_embedding_path, meta_path, distance_threshold=0)\n",
    "\n",
    "total_len = len(data)\n",
    "train_pct, val_pct, test_pct = 0.8, 0.1, 0.1\n",
    "train_len = int(train_pct * total_len)\n",
    "val_len = int(val_pct * total_len)\n",
    "test_len = total_len - train_len - val_len\n",
    "\n",
    "\n",
    "train_set, val_set, test_set = random_split(data, [train_len, val_len, test_len])\n",
    "\n",
    "# custom collate function for handling sequence padding (packed_pad_seq or something)\n",
    "def collate_fn(batch):\n",
    "    pass\n",
    "\n",
    "batch_size = 1  # can increase if GPU memory allows (sequences aready big (2-64), 384) shape\n",
    "\n",
    "# Train, test, validation\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTM_EMB_T(\n",
    "    sen_embedding_size=384,  # Embeddings are of size  384s\n",
    "    lstm_hidden_size=64,     # can customize\n",
    "    time_embedding_size=64,  # can customize\n",
    "    lstm_layer=1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCELoss()                # Binary cross entropy for 0/1 target\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4f9da",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
